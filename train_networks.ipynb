{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('compvis': conda)",
      "language": "python",
      "name": "python38364bitcompvisconda46954d3d832c4b2f84bde7a1c3a50552"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H5thgoDohupI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd58b1ba-7a68-4d03-9b0a-d35632be96a0"
      },
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bzxi4RNilhNj",
        "colab": {}
      },
      "source": [
        "# train_networks: Training CNNs to be used by the main program\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import imutils\n",
        "from processing import *\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "\n",
        "from skimage.segmentation import watershed\n",
        "from skimage.morphology import disk\n",
        "from skimage.feature import peak_local_max\n",
        "from skimage.filters import meijering\n",
        "\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "\n",
        "def plot_two_images(imgL, imgR, titleL, titleR):\n",
        "    f = plt.figure()\n",
        "    f.add_subplot(1,2, 1)\n",
        "    plt.imshow(imgL, cmap='gray')\n",
        "    plt.title(titleL)\n",
        "    f.add_subplot(1,2, 2)\n",
        "    plt.imshow(imgR, cmap='gray')\n",
        "    plt.title(titleR)\n",
        "    plt.show(block=True)\n",
        "    \n",
        "\n",
        "def load_data(dataset):\n",
        "    data = []\n",
        "    paths = [os.path.join(dataset, '01'), os.path.join(dataset, '02')]\n",
        "    for path in paths:\n",
        "        mask_path = path + '_ST'\n",
        "        mask_path = os.path.join(mask_path, 'SEG')\n",
        "        for f in os.listdir(mask_path):\n",
        "            if not f.endswith(\".tif\"):\n",
        "                continue\n",
        "            image = cv2.imread(os.path.join(path, f.replace('man_seg', 't')), cv2.IMREAD_GRAYSCALE)\n",
        "            image = equalize_clahe(image).astype(np.float32)\n",
        "            mask = cv2.imread(os.path.join(mask_path, f), cv2.IMREAD_UNCHANGED)\n",
        "            print(\"   Loaded \" + os.path.join(mask_path, f) + \", \" + os.path.join(path, f.replace('man_seg', 't')))\n",
        "            \n",
        "            # Generate the Cell Mask and Markers from the Mask\n",
        "            cell_mask = (mask > 0).astype(np.uint8)\n",
        "            markers = (get_markers(mask) > 0).astype(np.uint8)\n",
        "            weight_map = get_weight_map(markers)\n",
        "            \n",
        "            # Pack the data for the DataLoader\n",
        "            target = (cell_mask, markers, weight_map)\n",
        "            data.append((np.array([image]), target))\n",
        "\n",
        "    train_size = int(0.8 * len(data))\n",
        "    test_size = len(data) - train_size\n",
        "    train_data, test_data = random_split(data, [train_size, test_size])\n",
        "    trainLoader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
        "    testLoader = DataLoader(test_data, batch_size=5, shuffle=True)\n",
        "    return trainLoader, testLoader\n",
        "\n",
        "# def load_data(dataset):\n",
        "#     data = []\n",
        "#     path = os.path.join(dataset, \"originals\")\n",
        "#     clahe_path = path.replace(\"originals\", \"clahes\")\n",
        "#     mask_path = path.replace(\"originals\", \"masks\")\n",
        "#     markers_path = path.replace(\"originals\", \"markers\")\n",
        "#     wm_path = path.replace(\"originals\", \"weight_maps\")\n",
        "    \n",
        "#     for f in os.listdir(path):\n",
        "#         if not f.endswith(\".npy\"):\n",
        "#             continue\n",
        "# #         image = cv2.imread(os.path.join(path, f), cv2.IMREAD_GRAYSCALE)\n",
        "# #         clahe = cv2.imread(os.path.join(clahe_path, f), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "# #         cell_mask = cv2.imread(os.path.join(mask_path, f), cv2.IMREAD_UNCHANGED)\n",
        "# #         markers = cv2.imread(os.path.join(markers_path, f), cv2.IMREAD_UNCHANGED)\n",
        "# #         weight_map = cv2.imread(os.path.join(wm_path, f), cv2.IMREAD_UNCHANGED)\n",
        "        \n",
        "#         image = np.load(os.path.join(path, f))\n",
        "#         clahe = np.load(os.path.join(clahe_path, f))\n",
        "#         cell_mask = np.load(os.path.join(mask_path, f))\n",
        "#         markers = np.load(os.path.join(markers_path, f))\n",
        "#         weight_map = np.load(os.path.join(wm_path, f))\n",
        "#         print(\"   Loaded \" + os.path.join(mask_path, f) + \", \" + os.path.join(path, f.replace('mask', '')))\n",
        "        \n",
        "#         # Generate the Cell Mask and Markers from the Mask\n",
        "# #         cell_mask = (mask > 0).astype(np.uint8)\n",
        "# #         markers = (preprocessing.get_markers(mask) > 0).astype(np.uint8)\n",
        "# #         weight_map = preprocessing.get_weight_map(markers)\n",
        "\n",
        "# #         weight_map = []\n",
        "#         # Pack the data for the DataLoader\n",
        "#         target = (cell_mask, markers, weight_map)\n",
        "#         data.append((np.array([clahe]), target))\n",
        "            \n",
        "#     return DataLoader(data, batch_size=5, shuffle=True)\n",
        "\n",
        "def get_score(outputs, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates Accuracy Score across the batch\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    batch_size = outputs.shape[0]\n",
        "    total = outputs.shape[1] * outputs.shape[2]\n",
        "    for sample in range(batch_size):\n",
        "        num_correct = torch.sum(outputs[sample] == ground_truth[sample]).item()\n",
        "        score += float(num_correct) / total\n",
        "\n",
        "    return score / batch_size"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j2_SKo6FlhNt",
        "colab": {}
      },
      "source": [
        "# Class for creating the CNN\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.conv7 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv9 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv10 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv11 = nn.Conv2d(768, 256, 3, padding=1)\n",
        "        self.conv12 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv13 = nn.Conv2d(384, 128, 3, padding=1)\n",
        "        self.conv14 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.conv15 = nn.Conv2d(192, 64, 3, padding=1)\n",
        "        self.conv16 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.conv17 = nn.Conv2d(96, 32, 3, padding=1)\n",
        "        self.conv18 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.conv_out = nn.Conv2d(32, 2, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv1(x))\n",
        "        contraction_32 = F.relu(self.conv2(x))\n",
        "        \n",
        "        x = F.max_pool2d(contraction_32, kernel_size=2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        contraction_64 = F.relu(self.conv4(x))\n",
        "        \n",
        "        x = F.max_pool2d(contraction_64, kernel_size=2)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        contraction_128 = F.relu(self.conv6(x))\n",
        "        \n",
        "        x = F.max_pool2d(contraction_128, kernel_size=2)\n",
        "        x = F.relu(self.conv7(x))\n",
        "        contraction_256 = F.relu(self.conv8(x))\n",
        "        \n",
        "        x = F.max_pool2d(contraction_256, kernel_size=2)\n",
        "        x = F.relu(self.conv9(x))\n",
        "        x = F.relu(self.conv10(x))\n",
        "        \n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "        x = torch.cat((contraction_256, x), dim=1)\n",
        "        x = F.relu(self.conv11(x))\n",
        "        x = F.relu(self.conv12(x))\n",
        "        \n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "        x = torch.cat((contraction_128, x), dim=1)\n",
        "        x = F.relu(self.conv13(x))\n",
        "        x = F.relu(self.conv14(x))\n",
        "        \n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "        x = torch.cat((contraction_64, x), dim=1)\n",
        "        x = F.relu(self.conv15(x))\n",
        "        x = F.relu(self.conv16(x))\n",
        "        \n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
        "        x = torch.cat((contraction_32, x), dim=1)\n",
        "        x = F.relu(self.conv17(x))\n",
        "        x = F.relu(self.conv18(x))\n",
        "        \n",
        "        x = self.conv_out(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "# def weighted_cross_entropy_loss(inputs, targets, weights):\n",
        "\n",
        "# #     Weighted Cross-Entropy Loss takes in a weight map\n",
        "# #     and computes loss\n",
        "\n",
        "#     device = torch.device(\"cpu\")\n",
        "#     inputs = inputs.to(device)\n",
        "#     targets = targets.to(device)\n",
        "#     weights = weights.to(device)\n",
        "#     loss = torch.zeros(inputs.shape[0])\n",
        "\n",
        "#     # Calculate loss for each sample in the batch\n",
        "#     for sample in range(inputs.shape[0]):\n",
        "#         print(\"Sample\", sample+1)\n",
        "#         sample_loss, total_weight = 0.0, 0.0\n",
        "#         for row in range(inputs.shape[2]):\n",
        "#             for col in range(inputs.shape[3]):\n",
        "#                 # Get pixel q\n",
        "#                 q = (row, col)\n",
        "#                 w = weights[sample][q].item()\n",
        "#                 total_weight += w\n",
        "#                 if targets[sample][0][q] == 0:\n",
        "#                     # Get predicted probability for q = 0\n",
        "#                     p = inputs[sample][0][q]\n",
        "#                 else:\n",
        "#                     # Get predicted probability for q = 1\n",
        "#                     p = inputs[sample][1][q]\n",
        "#                 sample_loss -= w * torch.log(p).item()\n",
        "#         sample_loss = sample_loss / total_weight\n",
        "#         loss[sample] = sample_loss\n",
        "\n",
        "#     return torch.mean(loss)\n",
        "\n",
        "# '''\n",
        "def weighted_cross_entropy_loss(inputs, targets, weights):\n",
        "\n",
        "#     Weighted Cross-Entropy Loss takes in a weight map\n",
        "#     and computes loss\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('.', end='')\n",
        "    \n",
        "#     device = torch.device(\"cpu\")\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    weights = weights.to(device)\n",
        "    loss = torch.zeros(inputs.shape[0])\n",
        "\n",
        "    # Calculate loss for each sample in the batch\n",
        "    for sample in range(inputs.shape[0]):\n",
        "        # print(\"Sample\", sample+1)\n",
        "        sample_loss, total_weight = 0.0, 0.0\n",
        "        \n",
        "        # P(y(q))q\n",
        "        # log_p = torch.where( (targets[sample] < 0), torch.zeros(targets.shape), targets[sample])\n",
        "        log_p = torch.where( (targets[sample] == 0), inputs[sample][0], inputs[sample][1])\n",
        "        # print(torch.unique(inputs[sample][0]))\n",
        "        \n",
        "        log_pw = log_p * weights[sample]\n",
        "        \n",
        "        sum_log_pw = torch.sum(log_pw)\n",
        "        sum_weights = torch.sum(weights[sample])\n",
        "        # print(log_p,  sum_log_pw, sum_weights)\n",
        "\n",
        "        div_sums = sum_log_pw / sum_weights\n",
        "        \n",
        "        sample_loss = div_sums * -1\n",
        "        # print(div_sums, sample_loss)\n",
        "\n",
        "        loss[sample] = sample_loss\n",
        "\n",
        "    return torch.mean(loss)\n",
        "# '''\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Train 2 networks for predicting markers and the cell mask respectively\n",
        "    Set trains on data from \"Sequence 1 Masks\" and \"Sequence 2 Masks\"\n",
        "    and save the models\n",
        "    \"\"\"    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device: \" + str(device))\n",
        "    \n",
        "    # Net M predicts the markers. Net C predicts the cell mask\n",
        "    net_m, net_c = Network().to(device), Network().to(device)\n",
        "    \n",
        "    # criterion = F.nll_loss\n",
        "    criterion = weighted_cross_entropy_loss\n",
        "    \n",
        "    # Optimising using Adam algorithm\n",
        "    optimiser_m = optim.Adam(net_m.parameters(), lr=0.001)\n",
        "    optimiser_c = optim.Adam(net_c.parameters(), lr=0.001)\n",
        "    \n",
        "    # Iterate over a number of epochs on the data\n",
        "    for epoch in range(100):\n",
        "        for i, batch in enumerate(trainLoader):\n",
        "            x = batch[0].to(device)\n",
        "            target = batch[1]\n",
        "            cell_masks, markers = target[0].to(device), target[1].to(device) # Unpack target data\n",
        "            weight_map = target[2].to(device)\n",
        "\n",
        "            # Clear gradients from last step\n",
        "            optimiser_m.zero_grad()\n",
        "            optimiser_c.zero_grad()\n",
        "\n",
        "            # Predict the markers from the image\n",
        "            output_m = net_m(x)\n",
        "            # loss_m = criterion(output_m, markers.long())\n",
        "            loss_m = criterion(output_m, markers.float(), weight_map)\n",
        "            loss_m.backward()\n",
        "            optimiser_m.step()\n",
        "            \n",
        "            # Predict the Cell Mask from the image\n",
        "            output_c = net_c(x)\n",
        "            loss_c = criterion(output_c, cell_masks.float(), weight_map)\n",
        "            # loss_c = criterion(output_c, cell_masks.long())\n",
        "            loss_c.backward()\n",
        "            optimiser_c.step()\n",
        "\n",
        "            if i == 0 or (i + 1) % 10 == 0:\n",
        "                print(f\"Epoch: {epoch+1}, Batch: {i + 1}\")\n",
        "                print(f\"Cell Mask Loss: {loss_c.item():.2f}, Markers Loss: {loss_m.item():.2f}\")\n",
        "                \n",
        "                plt.imshow(x[0][0].cpu(), cmap='gray')\n",
        "                plt.title(\"Input\")\n",
        "                plt.show()\n",
        "\n",
        "                # Get the predicted Cell Mask and Markers for one of the images\n",
        "                pred_c = torch.argmax(output_c[0], dim=0).cpu()\n",
        "                pred_m = torch.argmax(output_m[0], dim=0).cpu()\n",
        "                \n",
        "                # Compare predicted to true images\n",
        "                plot_two_images(pred_c, cell_masks[0].cpu(), \"Predicted Cell Mask\", \"True Cell Mask\")\n",
        "                plot_two_images(pred_m, markers[0].cpu(), \"Predicted Markers\", \"True Markers\")\n",
        "\n",
        "\n",
        "        # Test on the evaluation set\n",
        "        print(\"\\n--- Evaluation ---\")\n",
        "        net_m.eval()\n",
        "        net_c.eval()\n",
        "        with torch.no_grad():\n",
        "            running_score = np.array([0.0, 0.0])\n",
        "            for i, batch in enumerate(testLoader):\n",
        "                x = batch[0].to(device)\n",
        "                target = batch[1]\n",
        "                cell_masks, markers = target[0].to(device), target[1].to(device) # Unpack target data\n",
        "                weight_map = target[2].to(device)\n",
        "\n",
        "                output_c = net_c(x)\n",
        "                output_m = net_m(x)\n",
        "\n",
        "                pred_c = torch.argmax(output_c, dim=1)\n",
        "                pred_m = torch.argmax(output_m, dim=1)\n",
        "                running_score[0] += get_score(pred_c, cell_masks)\n",
        "                running_score[1] += get_score(pred_m, markers)\n",
        "\n",
        "                if i == 0:\n",
        "                    plt.imshow(x[0][0].cpu(), cmap='gray')\n",
        "                    plt.title(\"Input\")\n",
        "                    plt.show()\n",
        "\n",
        "                    # Compare predicted to true images\n",
        "                    plot_two_images(pred_c[0].cpu(), cell_masks[0].cpu(), \"Predicted Cell Mask\", \"True Cell Mask\")\n",
        "                    plot_two_images(pred_m[0].cpu(), markers[0].cpu(), \"Predicted Markers\", \"True Markers\")\n",
        "\n",
        "            score = running_score / len(testLoader)\n",
        "            print(f\"EPOCH {epoch+1} SCORE\\nCell Mask: {score[0]:.3f}, Markers: {score[1]:.3f}\")\n",
        "            print(f\"Overall: {(score[0]+score[1])/2:.3f}\\n\\n\")\n",
        "        net_m.train()\n",
        "        net_c.train()\n",
        "\n",
        "    torch.save(net_m.state_dict(), \"./CNN_m.pth\")\n",
        "    torch.save(net_c.state_dict(), \"./CNN_c.pth\")\n",
        "    print(\"Saved models.\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j4icmg954Wpa",
        "colab": {}
      },
      "source": [
        "print(\"Loading Data...\")\n",
        "trainLoader, testLoader = load_data('DIC-2')\n",
        "print(\"Finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3L0jzi4lhN3",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}